## A Fast Learning Algorithm for Deep Belief Nets	

提出了利用RBM（受限玻尔兹曼机）、contrastive divergence（对比散度）算法、up-down算法等方法对网络进行逐层训练和微调，解决多层网络训练困难的问题。

### 1  论文的结构(简要概括)

#### 1.1  Abstract

说明了explaining away的存在会使具有多层的belief net的训练变得困难，提出利用complementary prior消除这种现象。推出了一个greedy algorithm，可以一次只学习belief net的一层，并利用wake-sleep算法的对比版本——up-down算法对网络进行微调。最后，实现了一个只有三层的神经网络对mnist数据集的识别。

- 作者想解决什么问题？

  explaining away的存在使得具有多层的网络的训练难度加大，作者尝试解决多层网络训练困难的问题

- 作者通过什么理论/模型来解决这个问题？

  提出了complementary prior（互补先验）来消除explaining away，说明了具有tied-weight的无限定向网络能够实现complementary prior，证明了RBM与具有tied-weight的无限定向网络是等价的。并提出了一种greedy algorithm来对网络进行一次一层的训练，并利用wake-sleep算法的对比版本——up-down算法对网络进行微调。

- 作者给出的答案是什么？

  作者通过在一个只有三层的网络对mnist数据集的识别来证明complementary prior（RBM）是可以有效解决多层网络的explaining away效应的。对mnist数据集的识别错误率要略优于当时比较流行的SVM等算法。

#### 1.2 Introduction

- 作者为什么研究这个课题？

  在当时，对多层网络的训练是很困难的。较难根据给定的输入向量去推断隐藏单元的条件分布。使得神经网络的发展陷入瓶颈。

- 目前这个课题的研究进行到了哪一阶段？存在哪些缺陷？作者是想通过本文解决哪个问题？

  尽管当时有许多如MCMC、变分法等方法来解决多层网络中的explaining away这一问题，但都无法取得一个很好的效果。作者尝试利用RBM的堆叠构建一个新的网络即deep belief nets（DBN网络）来解决explaining away。而到了现在，当时提出的DBN网络已经很少被使用了，但当时DBN网络的提出推动了之后神经网络的发展。

- 作者使用的理论是基于哪些假设？

  后验分布之所以非独立是因为有似然项，可以通过额外创建一个隐藏层，利用互补先验的方法，来消除该似然性。

#### 1.3 Related work

- 和作者这篇论文相关的工作有哪些？

  1992年，Radford Neal提出的logic belief net（有向图模型）（sigmoid belief net）

  1995年，Hiton等人提出的wake-sleep算法

  2002年Hiton提出的contrastive divergence（对比散度）算法

  BM（玻尔兹曼机）/RBM（受限玻尔兹曼机）的提出

- 之前工作的优缺点是什么？

  之前的工作被Hiton被综合应用于对deep belief net的构建当中。

- 作者主要是对之前的哪个工作进行改进？

  作者将之前这些工作结合在一起，证明了将多个RBM进行堆叠后，可以通过CD算法、up-down算法来对所构建的DBN进行训练和微调的，最后可以实现完成对多层网络的训练

#### 1.4  Theoretical Analysis

- 作者是用什么理论证明了自己的方法在理论上也是有保障的？

  作者通过公式推导证明具有tied weight的无限有向模型和RBM是等价的，能够实现complementary prior，消除explaining away

#### 1.5 Experiment

- 作者是在哪些数据集或者说场景下进行了测试？

  在minst 手写数字集上进行测试。

- 实验中的重要指标有哪些？

  实验中的重要指标为对mnist手写数字集的识别的错误率。

- 文章提出的方法在哪些指标上表现好？在哪些指标上表现不好？

  在对mnist数据集的识别错误率上，新构建的DBN略优于当时流行的SVM(支持向量机)、最邻近算法等，但优势并不是十分明显。


#### 1.6 Conclusion

- 这篇论文最大的贡献是什么？

  实现了对深度的多层的网络的训练。尝试构建了DBN，从理论和实践证明，对具有多层的网络进行训练是可行的，推动之后神经网络的发展。

- 论文中的方法还存在什么问题？

  在测试网络时，如果使用向上传递的图像来固定最顶部两层（既具备joint memory的两层）的下一层的500个单元的二进制状态，则错误率会比之前高出1%

- 作者觉得还可以怎么改进？

  作者认为可以先修复具备joint memory的层级的下一层的500个单元的二进制状态，再依次打开每个标签单元。该方法的错误率比训练网络时高出0.5%

### 2 论文想要解决的问题？

#### 2.1 背景是什么？

自神经网络在20世纪50年代发明以来，得到短暂的迅猛发展，但由于随着网络层数的增多，对网络的训练愈发困难，使得神经网络的发展遇到瓶颈。

#### 2.2 之前的方法存在哪些问题

之前的方法如MCMC（马尔可夫链蒙特卡洛方法）可以用于在后验分布中采样，但耗时太多；变分法（Neal & Hinton, 1998）只能对真实的分布进行简单的近似，且近似效果较差。

#### 2.3 输入和输出是什么？

DBN可以简单理解为由多个RBM堆叠形成的网络，而最上面的两层为无向连接，其余层为有向连接。可认为该网络的输入和输出与普通的网络的输入与输出近似。如DBN可以用于分类问题中，则其输入可以为一些待分类对象的特征，输出则为分类的结果。

### 3 论文研究的是否是一个新问题

在当时看来，具有稠密层数的网络难以训练并不是一个新的问题，其制约着神经网络的发展。

### 4 论文试图验证的科学假设

这篇论文试图证明complementary prior是可以有效解决explaining away的。试图证明具有tied-weight的无限定向网络与RBM是等价的。

### 5 相关的关键人物与工作

#### 5.1 之前存在哪些相关的工作

1992年，Radford Neal提出的logic belief net（有向图模型）（sigmoid belief net）

1995年，Hiton等人提出的wake-sleep算法

2002年Hiton提出的contrastive divergence（对比散度）算法

BM（玻尔兹曼机）/RBM（受限玻尔兹曼机）的提出

#### 5.2 本文是对哪个工作进行的改进

这篇论文在当时比较具有独创性，是对之前工作的综合应用。

#### 5.3 这个领域的关键研究者

Geoffery Hinton、Radford Neal、Simon Osindero、Yee-Whye Teh等人

### 6 论文提出的解决方案的关键

主要是证明了具有tied-weight的无限定向网络和RBM是等价的，能够实现complementary prior，从而消除explaining away，实现对多层网络的训练。

### 7 论文的解决方案有完备的理论证明吗

这篇论文运用了充分的理论推导证明了具有tied-weight的无限定向网络和RBM是等价的。

### 8 实验设计

#### 8.1用到了哪些数据集

主要用到了比较经典的mnist数据集。

#### 8.2与什么算法进行了比较

mnist数字识别任务上各种学习算法的错误率

![image.png](C:\Users\fan\Documents\WeChat Files\wxid_h21n3s7m58ib22\FileStorage\File\2021-01\论文笔记模板.assets\24439865-f9f6c97bc4080432.png)

与支持向量机（SVM）、反向传播算法、最邻近算法等进行了比较

#### 8.3评价指标是什么

评价指标为对mnist数据集手写数字的分类准确率，并将其与当时流行的其他算法进行比较。

#### 8.4有没有什么独特的实验设计？

### 9 实验支撑

#### 9.1 论文的数据集哪里获取

https://blog.csdn.net/songbinxu/article/details/82992264

1.可从Yann LeCun教授的主页下载：http://yann.lecun.com/exdb/mnist/

2.谷歌下载链接：https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz

3.通过tensorflow、keras获取

#### 9.2 源代码哪里可以获取

这篇论文的附录B即为相关的伪代码

#### 9.3 关键代码的讲解

![image-20210127183628298](C:\Users\fan\Desktop\repo_深度学习\2.A Fast Learning Algorithm for Deep Belief Nets.assets\image-20210127183628298.png)

![image.png](C:\Users\fan\Desktop\repo_深度学习\10.A Fast Learning Algorithm for Deep Belief Nets.assets\24439865-0e37c9abdce30c0d.png)

![image.png](C:\Users\fan\Desktop\repo_深度学习\10.A Fast Learning Algorithm for Deep Belief Nets.assets\24439865-c7965624cb584b91.png)

![image.png](C:\Users\fan\Desktop\repo_深度学习\10.A Fast Learning Algorithm for Deep Belief Nets.assets\24439865-cfd927fe5705658b.png)

![image-20210126161125788](C:\Users\fan\Desktop\repo_深度学习\10.A Fast Learning Algorithm for Deep Belief Nets.assets\image-20210126161125788.png)

### 10 实验结果是否验证了科学假设？

从mnist数据集的分类准确率来看，实验是成功的，证明了作者所提出的将多个RBM堆叠形成的DBN是有效的，可以消除explaining away，能够完成对具有稠密连接的网络即多层网络的训练

### 11 论文最大的贡献

解决了当时阻碍神经网络发展的“稠密连接的网络难以训练”的问题，推动了之后神经网络的发展。

### 12 论文的不足之处

这篇论文在当时具有较大的启发意义，独创性较强，但所提出的网络模型应用领域仍然比较局限，且优势相较于当时流行的其他算法并不明显。

#### 12.1 这篇论文之后的工作有哪些其他的改进

之后Hiton等人发表了《Reducing *the* *Dimensionality* of Data with Neural Networks》是对DBN网络的进一步完善和应用，引领了之后的深度学习的发展。

#### 12.2你觉得可以对这篇论文有什么改进

### 13 重要的相关论文

1. The wake-sleep algorithm for unsupervised neural networks  

2. An Introduction to Restricted Boltzmann Machines

3. Training Products of Experts by Minimizing Contrastive Divergence
4. Reducing *the* *Dimensionality* of Data with Neural Networks

### 14 不懂之处

1. complementary prior能够消除explaining away的原理不理解
2. 具有tied-weight的无限定向网络与RBM等价的原因不理解