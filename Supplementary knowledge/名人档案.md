# 名人档案

## 1 Li Fei-Fei （李飞飞）

现为美国斯坦福大学计算机科学系教授、美国国家工程院院士、以人为本人工智能研究院（HAI）院长、AI4ALL联合创始人及主席 、Twitter公司董事会独立董事，主要研究方向为机器学习、计算机视觉、认知计算神经学，曾获斯隆研究奖计算机科学奖、影响世界华人大奖，入选2015年“全球百大思想者”、“2017年度中国留学人员50人榜单”。2015年在TED的一段演讲“[李飞飞：如何教计算机理解图片](https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures?language=zh-cn)”很火爆。

<img src="https://bkimg.cdn.bcebos.com/pic/c75c10385343fbf28136b56dba7eca8065388f61?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U5Mg==,g_7,xp_5,yp_5" style="zoom:60%;" />

最突出的贡献：构建了**[ImageNet]([http://www.image-net.org/])**数据集

> **李飞飞在数据方面的研究改变了人工智能研究的形态，从这个意义上讲完全称得上是「改变了世界」。**

- **ImageNet:**

ImageNet项目由李飞飞教授于2007年发起，其团队花费两年半的时间才完成了一个含有**1500万张照片、涵盖22000种物品**的数据库，在2009年发表了一篇名为《ImageNet: A Large-Scale Hierarchical Image Database》的论文并免费公开数据集，但当时并没有激起很大的浪花甚至人们对更多的数据能改进算法这种简单的概念还相当怀疑。

ImageNet的重大转折点是**ImageNet挑战赛**：李飞飞说服著名的图像识别大赛 PASCAL VOC 举办方与 ImageNet 联合举办赛事，虽然 PASCAL 大赛虽然备受瞩目，数据集的质量很高，但类别却很少，只有 20 个，相比之下，ImageNet 的图像类别高达 1000个。随着比赛不断举办，ImageNet 与 PASAL 合作的这项赛事成为衡量图像分类算法在当时最复杂图像数据集上性能如何的一个基准。ImageNet挑战赛从2010年开始，到2017年不再举办，而后ImageNet由Kaggle公司继续维护。

> “人们惊讶地发现经 ImageNet 训练后的模型可以用作其它识别任务的启动模型。你可以先用 ImageNet 训练模型，然后再针对其它任务调试模型，这不仅仅是神经网络领域的突破，也是识别领域的重大进展。”
>
> **ImageNet 真正改变了人工智能领域对「数据」的认知，它让人们真正意识到数据集在 AI 研究中的核心地位，它和算法同等重要。**
>
> 参考：https://www.zhihu.com/question/30990652

## 2 Jia Deng

李飞飞的博士生，本科是清华大学计算机科学专业，现任密歇根大学计算机科学与工程系的助理教授，是Yahoo ACE奖，ICCV Marr奖和ECCV最佳论文奖的获得者。一直协助李飞飞运行**ImageNet** 项目，自2010年以来，协办ImageNet大规模视觉识别挑战赛（ILSVRC）直到2017年。是NIPS 2012和CVPR 2014 BigVision研讨会的主要组织者。

<img src="https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=1508345203,3500768900&fm=26&gp=0.jpg" style="zoom:60%;" />

代表性论文：
![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5jITwsOckc05dlP9nW8sEgqtgDY1fNcAyVojH4y41EAD5WOu5.SoUXnUEqhOjSHeHz7ib59XjstdNRKhXsEoga8!/b&bo=bQNdAgAAAAADBxM!&rf=viewer_4)

## 3 Geoffrey Everest Hinton

加拿大认知心理学家和计算机科学家，被誉为“神经网络之父”、“深度学习鼻祖”、”人工智能教父“。现任Google副总裁兼工程研究员、多伦多大学的特聘教授，也是Vector Institute首席科学顾问。2018年因作为“深度学习领域的三大先驱之一”获得图灵奖，被选为2017年改变全球商业格局的50人。

<img src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1605774694262&di=bb2f1207f539a8cb6858663af5d1a641&imgtype=0&src=http%3A%2F%2Fpic2.zhimg.com%2F50%2Fv2-1d49524120dcde04c5f00068cb78b810_hd.jpg" style="zoom:80%;" />

主要贡献：率先将**反向传播(Backpropagation)**用于多层神经网络，发明了**玻尔兹曼机(Boltzmann machine)**，提出**逐层初始化预训练方法**揭开了深度学习的序幕，提出了**胶囊神经网络(capsule network)**。

代表性论文：

1. 反向传播算法的使用

   Rumelhart D E, Hinton G E, Williams R J. Learning representations by  back-propagating errors[J]. Cognitive modeling, 1988, 5(3): 1. 

2. CNN语音识别开篇TDN网络

   Waibel A, Hanazawa T, Hinton G, et al. Phoneme recognition using  time-delay neural networks[J]. Backpropagation: Theory, Architectures  and Applications, 1995: 35-61. 

3. DBN网络的学习 

   Hinton G E, Osindero S, Teh Y W. A fast learning algorithm for deep  belief nets[J]. Neural computation, 2006, 18(7): 1527-1554. 

4. 深度学习的开篇

   Hinton G E, Salakhutdinov R R. Reducing the dimensionality of data  with neural networks[J]. science, 2006, 313(5786): 504-507. 

5. 数据降维可视化方法t-SNE

   Maaten L, Hinton G. Visualizing data using t-SNE[J]. Journal of machine learning research, 2008, 9(Nov): 2579-2605. 

6. DBM模型

   Salakhutdinov R, Hinton G. Deep boltzmann machines[C]//Artificial intelligence and statistics. 2009: 448-455. 

7. **ReLU激活函数的使用**

   Nair V, Hinton G E. Rectified linear units improve restricted  boltzmann machines[C]//Proceedings of the 27th international conference  on machine learning (ICML-10). 2010: 807-814. 

8. RBM模型的训练

   Hinton G E. A practical guide to training restricted Boltzmann  machines[M]//Neural networks: Tricks of the trade. Springer, Berlin,  Heidelberg, 2012: 599-619. 

9. 深度学习语音识别开篇

   Hinton G, Deng L, Yu D, et al. Deep neural networks for acoustic modeling in speech recognition[J]. IEEE Signal  processing magazine, 2012, 29. 

10. **深度学习图像识别开篇AlexNet** 

    Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with  deep convolutional neural networks[C]//Advances in neural information  processing systems. 2012: 1097-1105. 

11. 权重初始化和Momentum优化方法的研究

    Sutskever I, Martens J, Dahl G, et al. On the importance of  initialization and momentum in deep learning[C]//International  conference on machine learning. 2013: 1139-1147. 

12. **Dropout方法提出** 

    Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: a simple way to  prevent neural networks from overfitting[J]. The Journal of Machine  Learning Research, 2014, 15(1): 1929-1958. 

13. 三巨头深度学习综述

    LeCun Y, Bengio Y, Hinton G. Deep learning[J]. nature, 2015, 521(7553): 436. 

14. 蒸馏学习算法

    Hinton G, Vinyals O, Dean J. Distilling the knowledge in a neural network[J]. arXiv preprint arXiv:1503.02531, 2015. 

15. Capsule NetworkSabour S, Frosst N, Hinton G E.

    Dynamic routing between  capsules[C]//Advances in neural information processing systems. 2017:  3856-3866. 

> 参考：https://www.sohu.com/a/328382912_120233360

## 4 Alex Krizhevsky

AlexNet论文的第一作者，Hinton的博士生。

![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1605776418461&di=8d8bde39b16b522f57010c020cdfd4e7&imgtype=0&src=http%3A%2F%2Fimages.ofweek.com%2FUpload%2FNews%2F2016-9%2FLiuzhongyang%2F9_16%2F6.jpg)

（从左到右依次为：Ilya Sutskever、Alex Krizhevsky、Geoffrey Hinton）

代表性论文：

![](http://m.qpic.cn/psc?/V50v5bPV1er4BO1DcJVb3iyeFd3aL3dx/ruAMsa53pVQWN7FLK88i5hsSmHqQZw7u5bYU9v9fChL2CP61natzGxjJzAt5LDNedoQsGNRjeOTcAyafdIxY25pCa1l3VZrPVNqyudnhNZg!/b&bo=SANXAgAAAAADBzw!&rf=viewer_4)

## 5 Yann LeCun

Hinton的博士后，CNN之父，纽约大学终身教授，前Facebook人工智能研究院负责人，IJCV、PAMI和IEEE Trans 的审稿人，创建了ICLR(International  Conference on Learning Representations)会议并且跟Yoshua Bengio共同担任主席。2014年获得了IEEE神经网络领军人物奖，2018荣获图灵奖。

![](https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=3425959714,1198945987&fm=15&gp=0.jpg)

主要贡献：1998年开发了**LeNet5**，并制作了被Hinton称为“机器学习界的果蝇”的经典数据集**MNIST**。

代表性论文：

1. 使用反向传播和神经网络识别手写数字

   LeCun Y, Boser B, Denker J S, et al. Backpropagation applied to  handwritten zip code recognition[J]. Neural computation, 1989, 1(4):  541-551. 

2. 早期权值剪枝的研究

   LeCun Y, Denker J S, Solla S A. Optimal brain damage[C]//Advances in neural information processing systems. 1990: 598-605. 

3. 将siamese网络用于签名验证

   Bromley J, Guyon I, LeCun Y, et al. Signature verification using a"  siamese" time delay neural network[C]//Advances in neural information  processing systems. 1994: 737-744. 

4. LeNet5卷积神经网络提出

   LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):  2278-2324. 

5. 对**max pooling**和average pooling的理论分析

   Boureau Y L, Ponce J, LeCun Y. A theoretical analysis of feature  pooling in visual recognition[C]//Proceedings of the 27th international  conference on machine learning (ICML-10). 2010: 111-118. 

6. DropConnect方法

   Wan L, Zeiler M, Zhang S, et al. Regularization of neural networks  using dropconnect[C]//International conference on machine learning.  2013: 1058-1066. 

7. OverFeat检测框架

   Sermanet P, Eigen D, Zhang X, et al. Overfeat: Integrated recognition, localization and detection using convolutional networks[J]. arXiv  preprint arXiv:1312.6229, 2013. 

8. CNN用于立体匹配

   Zbontar J, LeCun Y. Computing the stereo matching cost with a  convolutional neural network[C]//Proceedings of the IEEE conference on  computer vision and pattern recognition. 2015: 1592-1599. 

9. 三巨头深度学习综述

   LeCun Y, Bengio Y, Hinton G. Deep learning[J]. nature, 2015, 521(7553): 436. 

10. EBGAN

    Zhao J, Mathieu M, LeCun Y. Energy-based generative adversarial network[J]. arXiv preprint arXiv:1609.03126, 2016. 

> 参考：https://www.sohu.com/a/328598636_120233360